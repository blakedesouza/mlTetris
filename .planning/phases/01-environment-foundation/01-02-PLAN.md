---
phase: 01-environment-foundation
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - src/environment/tetris_env.py
  - src/environment/config.py
  - src/environment/__init__.py
  - tests/__init__.py
  - tests/environment/__init__.py
  - tests/environment/test_env_creation.py
  - tests/environment/test_observations.py
  - tests/environment/test_actions.py
  - tests/environment/test_rewards.py
  - tests/environment/test_gymnasium_api.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Environment can be instantiated and reset via Gymnasium API"
    - "Observations are feature vectors (column heights, holes, bumpiness)"
    - "Agent can select final piece placements (rotation + column) as single actions"
    - "Rewards penalize holes/height and reward line clears"
    - "Environment passes Gymnasium check_env validation"
  artifacts:
    - path: "src/environment/tetris_env.py"
      provides: "Environment factory function"
      exports: ["create_tetris_env", "make_env"]
      min_lines: 30
    - path: "src/environment/config.py"
      provides: "Environment configuration dataclass"
      exports: ["EnvConfig"]
    - path: "tests/environment/test_gymnasium_api.py"
      provides: "Gymnasium API compliance tests"
      contains: "check_env"
  key_links:
    - from: "src/environment/tetris_env.py"
      to: "tetris_gymnasium.wrappers.GroupedActionsObservations"
      via: "import and wrapper application"
      pattern: "GroupedActionsObservations"
    - from: "src/environment/tetris_env.py"
      to: "tetris_gymnasium.wrappers.FeatureVectorObservation"
      via: "import and wrapper application"
      pattern: "FeatureVectorObservation"
    - from: "src/environment/tetris_env.py"
      to: "src/environment/wrappers/shaped_reward.py"
      via: "import ShapedRewardWrapper"
      pattern: "from.*wrappers.*import.*ShapedRewardWrapper"
---

<objective>
Create the environment factory and validate against all Phase 1 requirements.

Purpose: Deliver the complete Gymnasium-compatible Tetris environment with feature-based observations, meta-actions, and shaped rewards. This plan wires everything together and proves it works.

Output: Working environment factory that passes check_env and fulfills ENV-01 through ENV-05.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-environment-foundation/01-RESEARCH.md
@.planning/phases/01-environment-foundation/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create environment factory with proper wrapper stacking</name>
  <files>
    src/environment/config.py
    src/environment/tetris_env.py
    src/environment/__init__.py
  </files>
  <action>
    1. Create src/environment/config.py:
       ```python
       from dataclasses import dataclass
       from typing import Optional

       @dataclass
       class EnvConfig:
           """Configuration for Tetris environment creation."""
           render_mode: Optional[str] = None  # None, "human", "ansi", "rgb_array"
           hole_penalty: float = -0.5
           height_penalty: float = -0.1
           clear_bonus_multiplier: float = 1.0
           game_over_penalty: float = -10.0
       ```

    2. Create src/environment/tetris_env.py with two functions:

       create_tetris_env(render_mode=None, reward_config=None):
         """Create fully-configured Tetris environment for RL training."""
         - Create base env: gym.make("tetris_gymnasium/Tetris", render_mode=render_mode)
         - Apply wrappers IN THIS ORDER (order matters!):
           1. GroupedActionsObservations (meta-actions)
           2. FeatureVectorObservation (feature extraction)
           3. ShapedRewardWrapper (reward shaping)
         - Return wrapped environment

       make_env(config: EnvConfig = None):
         """Factory that returns a callable for vectorized env creation."""
         - If config is None, use default EnvConfig()
         - Return a _init() function that calls create_tetris_env with config settings
         - This pattern is needed for SB3's make_vec_env in Phase 2

    3. Update src/environment/__init__.py to export:
       - create_tetris_env
       - make_env
       - EnvConfig
       - ShapedRewardWrapper (re-export from wrappers)

    CRITICAL: Wrapper order must be:
      base -> GroupedActionsObservations -> FeatureVectorObservation -> ShapedRewardWrapper
    Applying feature extraction before meta-actions breaks observation generation.
  </action>
  <verify>
    Run Python to test basic creation:
    ```python
    from src.environment import create_tetris_env
    env = create_tetris_env()
    obs, info = env.reset()
    print(f"Obs shape: {obs.shape}, Action space: {env.action_space}")
    env.close()
    ```
  </verify>
  <done>
    Environment factory creates wrapped environment with feature observations and meta-actions.
  </done>
</task>

<task type="auto">
  <name>Task 2: Write comprehensive environment tests</name>
  <files>
    tests/__init__.py
    tests/environment/__init__.py
    tests/environment/test_env_creation.py
    tests/environment/test_observations.py
    tests/environment/test_actions.py
    tests/environment/test_rewards.py
    tests/environment/test_gymnasium_api.py
  </files>
  <action>
    Create test directory structure with __init__.py files.

    test_env_creation.py:
    - test_create_env_default: Can create with no args
    - test_create_env_with_render_mode: Can create with render_mode="ansi"
    - test_create_env_with_config: Can create with EnvConfig
    - test_env_reset: reset() returns (obs, info) tuple
    - test_env_step: step() returns 5-tuple (obs, reward, terminated, truncated, info)

    test_observations.py (ENV-02):
    - test_observation_is_feature_vector: obs is 1D numpy array, not 2D board
    - test_observation_shape_reasonable: Feature count is reasonable (10-20 for 10-wide board)
    - test_observation_dtype: obs dtype is float32 or float64
    - test_observation_in_space: obs is within observation_space bounds

    test_actions.py (ENV-03):
    - test_action_space_is_discrete: action_space is Discrete
    - test_action_space_size: Size is width * 4 = 40 (meta-actions for 10-wide board)
    - test_can_take_action: Taking action 0 doesn't crash
    - test_all_actions_valid: Can take each action in action_space (may use action mask)

    test_rewards.py (ENV-04):
    - test_reward_is_float: Reward from step is float
    - test_shaped_reward_config: Can configure reward coefficients
    - test_game_over_penalty: Terminated episode has game_over_penalty applied
    - test_reward_not_sparse: Rewards occur before line clears (from shaping)

    test_gymnasium_api.py (ENV-05):
    - test_check_env_passes: check_env(env, warn=True, skip_render_check=True) passes
    - test_reset_returns_tuple: reset returns (obs, info)
    - test_step_returns_five: step returns 5-tuple
    - test_observation_space_type: observation_space is Box
    - test_action_space_type: action_space is Discrete

    Use pytest fixtures for env creation:
    ```python
    import pytest
    from src.environment import create_tetris_env

    @pytest.fixture
    def env():
        e = create_tetris_env()
        yield e
        e.close()
    ```
  </action>
  <verify>
    Run: pytest tests/environment/ -v
    All tests should pass.
  </verify>
  <done>
    All tests pass, covering ENV-01 through ENV-05 requirements.
  </done>
</task>

<task type="auto">
  <name>Task 3: Run check_env validation and document environment specs</name>
  <files>
    (no new files, validation only)
  </files>
  <action>
    1. Run check_env explicitly and capture output:
       ```python
       from stable_baselines3.common.env_checker import check_env
       from src.environment import create_tetris_env

       env = create_tetris_env()
       check_env(env, warn=True, skip_render_check=True)
       print("check_env PASSED")
       ```

    2. Document the actual environment specs discovered:
       - Observation shape (should be ~13 features for 10-wide board)
       - Action space size (should be 40 for 10-wide * 4 rotations)
       - Observation space bounds

    3. Run one full episode with random actions to verify end-to-end:
       ```python
       env = create_tetris_env()
       obs, info = env.reset(seed=42)
       done = False
       steps = 0
       total_reward = 0
       while not done and steps < 1000:
           action = env.action_space.sample()
           obs, reward, terminated, truncated, info = env.step(action)
           total_reward += reward
           steps += 1
           done = terminated or truncated
       print(f"Episode: {steps} steps, reward: {total_reward}")
       env.close()
       ```

    If check_env fails:
    - Read the error message carefully
    - Common issues: observation outside bounds, dtype mismatch, missing methods
    - Fix in tetris_env.py or shaped_reward.py as needed
    - Re-run until check_env passes
  </action>
  <verify>
    check_env passes without exceptions.
    Full episode runs to completion.
  </verify>
  <done>
    Environment passes Gymnasium check_env validation and can run complete episodes.
  </done>
</task>

</tasks>

<verification>
After all tasks complete, verify all Phase 1 success criteria:

1. Environment instantiation (ENV-01, ENV-05):
   ```python
   from src.environment import create_tetris_env
   env = create_tetris_env()
   obs, info = env.reset()
   # Should work without errors
   ```

2. Feature-based observations (ENV-02):
   ```python
   assert len(obs.shape) == 1, "Observation should be 1D feature vector"
   assert obs.shape[0] > 5, "Should have multiple features"
   ```

3. Meta-actions (ENV-03):
   ```python
   from gymnasium.spaces import Discrete
   assert isinstance(env.action_space, Discrete)
   assert env.action_space.n == 40, "Should be width(10) * rotations(4)"
   ```

4. Shaped rewards (ENV-04):
   ```python
   # Run a few steps and check reward variety
   rewards = []
   for _ in range(10):
       _, r, _, _, _ = env.step(env.action_space.sample())
       rewards.append(r)
   assert len(set(rewards)) > 1, "Rewards should vary (not all zeros)"
   ```

5. check_env passes (ENV-05):
   ```python
   from stable_baselines3.common.env_checker import check_env
   check_env(env, warn=True, skip_render_check=True)
   ```
</verification>

<success_criteria>
- create_tetris_env() returns working Gymnasium environment
- Observation is 1D feature vector (not 2D board grid)
- Action space is Discrete(40) for meta-actions
- Rewards include shaping (not just sparse line clears)
- check_env passes without errors
- All pytest tests pass
- Can run complete episode with random policy
</success_criteria>

<output>
After completion, create `.planning/phases/01-environment-foundation/01-02-SUMMARY.md`
</output>
