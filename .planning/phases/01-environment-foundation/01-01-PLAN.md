---
phase: 01-environment-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - src/__init__.py
  - src/environment/__init__.py
  - src/environment/wrappers/__init__.py
  - src/environment/wrappers/shaped_reward.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Project can install dependencies via pip install -e ."
    - "ShapedRewardWrapper modifies rewards based on holes and height"
    - "Wrapper state resets correctly between episodes"
  artifacts:
    - path: "pyproject.toml"
      provides: "Project configuration with dependencies"
      contains: "tetris-gymnasium"
    - path: "src/environment/wrappers/shaped_reward.py"
      provides: "Custom reward shaping wrapper"
      exports: ["ShapedRewardWrapper"]
      min_lines: 50
  key_links:
    - from: "src/environment/wrappers/shaped_reward.py"
      to: "gymnasium.RewardWrapper"
      via: "class inheritance"
      pattern: "class ShapedRewardWrapper.*RewardWrapper"
---

<objective>
Set up the project structure and implement the custom ShapedRewardWrapper.

Purpose: Establish the foundation for the Tetris environment with proper package structure and the only custom code needed (reward shaping). tetris-gymnasium provides everything else via built-in wrappers.

Output: Installable Python package with ShapedRewardWrapper ready to use in environment factory.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-environment-foundation/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Initialize project structure and dependencies</name>
  <files>
    pyproject.toml
    src/__init__.py
    src/environment/__init__.py
    src/environment/wrappers/__init__.py
  </files>
  <action>
    Create pyproject.toml with:
    - name: "ml-tetris"
    - Python >=3.10
    - Dependencies: tetris-gymnasium==0.3.0, gymnasium==1.2.3, numpy>=1.24
    - Dev dependencies: stable-baselines3==2.7.1, pytest>=7.0
    - Package discovery pointing to src/

    Create src/__init__.py (empty, marks as package).
    Create src/environment/__init__.py that will export the main factory function (placeholder for now).
    Create src/environment/wrappers/__init__.py that exports ShapedRewardWrapper.

    Use modern pyproject.toml format (no setup.py needed).
  </action>
  <verify>
    Run: pip install -e ".[dev]"
    Should complete without errors.
  </verify>
  <done>
    Package installs successfully with all dependencies available.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement ShapedRewardWrapper</name>
  <files>
    src/environment/wrappers/shaped_reward.py
  </files>
  <action>
    Implement ShapedRewardWrapper based on research pattern:

    1. Inherit from gymnasium.RewardWrapper
    2. Constructor accepts configurable coefficients:
       - hole_penalty: float = -0.5 (per new hole created)
       - height_penalty: float = -0.1 (per height increase)
       - clear_bonus_multiplier: float = 1.0
       - game_over_penalty: float = -10.0

    3. Override reset() to:
       - Call parent reset()
       - Initialize _prev_holes and _prev_height from initial state
       - Return obs, info unchanged

    4. Override step() to:
       - Call parent step() to get obs, reward, terminated, truncated, info
       - Calculate current holes and max height from board state
       - Compute deltas: new_holes = max(0, current - prev), height_increase = max(0, current - prev)
       - Shape reward: shaped = (reward * clear_bonus_multiplier) + (new_holes * hole_penalty) + (height_increase * height_penalty)
       - Add game_over_penalty if terminated
       - Update _prev_holes, _prev_height
       - Return obs, shaped_reward, terminated, truncated, info

    5. Helper methods to extract holes and height:
       - _get_holes(info, obs): Try info['holes'] first, fall back to computing from board
       - _get_max_height(info, obs): Try info['max_height'] first, fall back to computing
       - Access unwrapped env's board if needed: self.env.unwrapped.board

    IMPORTANT: The reward() method from RewardWrapper is not directly used - we override step() to have access to both observation and info. This matches the pattern in the research doc.

    Add docstring explaining the reward formula:
    reward = (lines_cleared^2 * clear_bonus_multiplier)
           + (new_holes * hole_penalty)
           + (height_increase * height_penalty)
           + (game_over ? game_over_penalty : 0)
  </action>
  <verify>
    Run Python to check class imports:
    python -c "from src.environment.wrappers import ShapedRewardWrapper; print('OK')"
  </verify>
  <done>
    ShapedRewardWrapper class exists with configurable coefficients, proper reset/step overrides, and feature extraction helpers.
  </done>
</task>

</tasks>

<verification>
After both tasks complete:

1. Project structure exists:
   - src/environment/wrappers/shaped_reward.py
   - pyproject.toml with correct dependencies

2. Package installs: `pip install -e ".[dev]"` succeeds

3. Import works: `from src.environment.wrappers import ShapedRewardWrapper`

4. Class has expected interface:
   ```python
   from src.environment.wrappers import ShapedRewardWrapper
   import inspect
   sig = inspect.signature(ShapedRewardWrapper.__init__)
   assert 'hole_penalty' in sig.parameters
   assert 'height_penalty' in sig.parameters
   ```
</verification>

<success_criteria>
- pyproject.toml exists with tetris-gymnasium, gymnasium, numpy dependencies
- Package installs without errors
- ShapedRewardWrapper imports successfully
- Wrapper has configurable reward coefficients
- Wrapper properly inherits from gymnasium.RewardWrapper
</success_criteria>

<output>
After completion, create `.planning/phases/01-environment-foundation/01-01-SUMMARY.md`
</output>
