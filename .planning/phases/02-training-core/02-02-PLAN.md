---
phase: 02-training-core
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/training/callbacks.py
  - src/training/train.py
  - src/training/__init__.py
autonomous: true

must_haves:
  truths:
    - "GoalReachedCallback stops training when target lines achieved"
    - "train_headless() runs training without visualization"
    - "Periodic checkpoints saved during training"
    - "Training can resume from checkpoint directory if exists"
  artifacts:
    - path: "src/training/callbacks.py"
      provides: "GoalReachedCallback, LinesTrackingCallback"
      exports: ["GoalReachedCallback", "LinesTrackingCallback"]
    - path: "src/training/train.py"
      provides: "Headless training function"
      exports: ["train_headless"]
  key_links:
    - from: "src/training/callbacks.py"
      to: "stable_baselines3.common.callbacks.BaseCallback"
      via: "inheritance"
      pattern: "class GoalReachedCallback\\(BaseCallback\\)"
    - from: "src/training/train.py"
      to: "src/training/agent.py"
      via: "TetrisAgent import"
      pattern: "from .agent import TetrisAgent"
---

<objective>
Create training callbacks and headless training function for autonomous DQN training.

Purpose: Enable goal-based early stopping (TRAIN-04) and headless training (TRAIN-07) with automatic checkpoint management during training.

Output: GoalReachedCallback, train_headless() function with checkpoint support.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-training-core/02-RESEARCH.md
@.planning/phases/02-training-core/02-01-SUMMARY.md

# Dependencies from 02-01
@src/training/config.py
@src/training/agent.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create training callbacks</name>
  <files>
    src/training/callbacks.py
    src/training/__init__.py
  </files>
  <action>
Create src/training/callbacks.py with custom SB3 callbacks.

1. LinesTrackingCallback(BaseCallback):
   Purpose: Track lines cleared during training since tetris-gymnasium may not include 'lines' in ep_info_buffer automatically.

   Implementation:
   - Override _on_step()
   - Access env via self.training_env (VecEnv wrapper)
   - After each episode ends (check 'dones' in locals), extract lines from info if available
   - Track best_lines and total_lines as instance attributes
   - Provide get_best_lines() method

   Note: The environment may need to be inspected to find where lines are tracked. Check info dict keys during step.

2. GoalReachedCallback(BaseCallback):
   Purpose: Stop training when target lines cleared is achieved (TRAIN-04).

   Constructor:
   - target_lines: int (required)
   - lines_tracker: LinesTrackingCallback (required - for coordination)
   - check_freq: int = 1000 (how often to check)
   - verbose: int = 1

   Implementation:
   - In _on_step(), check if lines_tracker.best_lines >= target_lines
   - If goal reached, print message and return False to stop training
   - Return True to continue

3. CheckpointCallback wrapper (optional enhancement):
   If SB3's built-in CheckpointCallback doesn't save replay buffer correctly, create a wrapper that:
   - Inherits from CheckpointCallback
   - Overrides _on_step to also save replay buffer

   Actually, SB3 CheckpointCallback has save_replay_buffer=True option, so just document usage.

Update __init__.py to export callbacks.
  </action>
  <verify>
python -c "
from src.training.callbacks import LinesTrackingCallback, GoalReachedCallback
from stable_baselines3.common.callbacks import BaseCallback
assert issubclass(LinesTrackingCallback, BaseCallback)
assert issubclass(GoalReachedCallback, BaseCallback)
print('Callbacks OK')
"
  </verify>
  <done>
LinesTrackingCallback and GoalReachedCallback classes created and inherit from BaseCallback.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create train_headless function</name>
  <files>
    src/training/train.py
    src/training/__init__.py
  </files>
  <action>
Create src/training/train.py with headless training function.

train_headless(config: TrainingConfig, checkpoint_dir: Optional[str] = None) -> TetrisAgent:
    """Run headless training with periodic checkpoints.

    Args:
        config: Training configuration
        checkpoint_dir: Override for checkpoint directory (default: config.checkpoint_dir)

    Returns:
        Trained TetrisAgent
    """

Implementation:
1. Set checkpoint_dir from param or config.checkpoint_dir
2. Create environment:
   - Use make_env(EnvConfig(render_mode=None)) for headless
   - Call the factory to get actual env

3. Check for existing checkpoint:
   - If checkpoint_dir / "latest" exists, load agent from it
   - Print "Resuming from checkpoint, X steps already trained"
   - Otherwise create new TetrisAgent(env, config)

4. Set up callbacks list:
   - LinesTrackingCallback for tracking progress
   - SB3 CheckpointCallback(save_freq=config.checkpoint_freq, save_path=checkpoint_dir, name_prefix="ckpt", save_replay_buffer=True)
   - If config.target_lines is set, add GoalReachedCallback(target_lines, lines_tracker)
   - CallbackList to combine them

5. Calculate remaining timesteps:
   - remaining = config.max_timesteps - agent.timesteps_trained
   - If remaining <= 0, return agent (already done)

6. Train:
   - agent.train(remaining, callback=callback_list)

7. Save final checkpoint:
   - agent.save_checkpoint(checkpoint_dir / "latest")
   - Also save to checkpoint_dir / "final" for clarity

8. Return agent

Also create a simple CLI entry point (optional, in same file):
```python
if __name__ == "__main__":
    import argparse
    # Basic arg parsing for target_lines, max_timesteps, checkpoint_dir
    # Call train_headless with parsed config
```

Update __init__.py to export train_headless.
  </action>
  <verify>
python -c "
from src.training import train_headless, TrainingConfig
# Just verify import works - actual training tested in plan 3
print('train_headless importable')
"
  </verify>
  <done>
train_headless function created and importable, handles checkpoint resume logic.
  </done>
</task>

</tasks>

<verification>
```bash
# All imports work
python -c "from src.training import TrainingConfig, TetrisAgent, train_headless, GoalReachedCallback, LinesTrackingCallback"

# Callbacks can be instantiated (dry run)
python -c "
from src.training.callbacks import LinesTrackingCallback, GoalReachedCallback
lines_cb = LinesTrackingCallback()
goal_cb = GoalReachedCallback(target_lines=10, lines_tracker=lines_cb)
print('Callbacks instantiate OK')
"

# Short smoke test (very brief training to verify no crashes)
python -c "
from src.training import train_headless, TrainingConfig
import tempfile
import os

with tempfile.TemporaryDirectory() as tmpdir:
    config = TrainingConfig(max_timesteps=100, learning_starts=10, checkpoint_freq=50)
    agent = train_headless(config, checkpoint_dir=tmpdir)
    print(f'Trained for {agent.timesteps_trained} steps')
    assert os.path.exists(os.path.join(tmpdir, 'latest'))
    print('Checkpoint saved OK')
"
```
</verification>

<success_criteria>
- GoalReachedCallback stops training when target lines reached
- LinesTrackingCallback tracks lines cleared during training
- train_headless runs without visualization overhead
- Checkpoints saved periodically and at end
- Resume from checkpoint works (detected and loaded)
</success_criteria>

<output>
After completion, create `.planning/phases/02-training-core/02-02-SUMMARY.md`
</output>
