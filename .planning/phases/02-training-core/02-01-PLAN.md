---
phase: 02-training-core
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/training/__init__.py
  - src/training/config.py
  - src/training/agent.py
autonomous: true

must_haves:
  truths:
    - "TrainingConfig provides all DQN hyperparameters with Tetris-tuned defaults"
    - "TetrisAgent wraps SB3 DQN with unified checkpoint save/load"
    - "Checkpoint saves model weights, optimizer state, replay buffer, and metadata together"
    - "Agent can be loaded from checkpoint and continue training"
  artifacts:
    - path: "src/training/__init__.py"
      provides: "Module exports"
    - path: "src/training/config.py"
      provides: "TrainingConfig dataclass"
      contains: "class TrainingConfig"
    - path: "src/training/agent.py"
      provides: "TetrisAgent wrapper class"
      exports: ["TetrisAgent"]
  key_links:
    - from: "src/training/agent.py"
      to: "stable_baselines3.DQN"
      via: "self.model = DQN(...)"
      pattern: "DQN\\("
    - from: "src/training/agent.py"
      to: "src/training/config.py"
      via: "TrainingConfig import"
      pattern: "from .config import TrainingConfig"
---

<objective>
Create the training configuration and agent wrapper that forms the foundation for DQN training.

Purpose: Establish the core training infrastructure with proper checkpoint support that preserves full training state (model + replay buffer + metadata) for true resume capability.

Output: TrainingConfig dataclass and TetrisAgent class with save_checkpoint/load_checkpoint methods.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-training-core/02-RESEARCH.md

# Phase 1 provides environment factory
@src/environment/__init__.py
@src/environment/tetris_env.py
@src/environment/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create training module with TrainingConfig</name>
  <files>
    src/training/__init__.py
    src/training/config.py
  </files>
  <action>
Create src/training/ module with TrainingConfig dataclass.

TrainingConfig should include all DQN hyperparameters with Tetris-tuned defaults from research:
- learning_rate: 1e-4
- buffer_size: 100_000 (smaller than default 1M, sufficient for Tetris)
- batch_size: 64
- gamma: 0.99
- exploration_fraction: 0.2 (20% of training for exploration decay)
- exploration_initial_eps: 1.0
- exploration_final_eps: 0.05
- target_update_interval: 1000 (more frequent than default 10000)
- train_freq: 4
- gradient_steps: 1
- learning_starts: 10_000

Additional fields:
- target_lines: Optional[int] = None (for TRAIN-04 goal)
- max_timesteps: int = 500_000 (default training length)
- net_arch: tuple = (256, 256) (network architecture)
- checkpoint_dir: str = "./checkpoints" (where to save)
- checkpoint_freq: int = 10_000 (save every N steps)

Include to_dict() and from_dict() class methods for serialization.

In __init__.py, export TrainingConfig.
  </action>
  <verify>
python -c "from src.training import TrainingConfig; c = TrainingConfig(); print(c.learning_rate, c.buffer_size, c.target_lines)"
  </verify>
  <done>
TrainingConfig can be imported and instantiated with Tetris-tuned defaults.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create TetrisAgent wrapper with checkpoint support</name>
  <files>
    src/training/agent.py
    src/training/__init__.py
  </files>
  <action>
Create TetrisAgent class that wraps SB3 DQN with unified checkpoint support.

Key implementation details:

1. Constructor:
   - Takes env (Gymnasium env) and config (TrainingConfig)
   - Creates DQN with MlpPolicy using config parameters
   - Pass policy_kwargs=dict(net_arch=list(config.net_arch)) for network architecture
   - Track _total_timesteps_trained internally

2. train(total_timesteps, callback=None):
   - Calls model.learn() with reset_num_timesteps=False (continue counting)
   - Updates _total_timesteps_trained

3. save_checkpoint(path):
   - Creates directory if not exists
   - model.save(path / "model.zip")
   - model.save_replay_buffer(path / "replay_buffer.pkl")
   - Save metadata.json with: total_timesteps_trained, num_timesteps, config.to_dict()

4. load_checkpoint(path, env) classmethod:
   - Load metadata.json first
   - Create TrainingConfig from metadata["config"]
   - Use DQN.load(path / "model.zip", env=env) - NOT create then load
   - Load replay buffer with model.load_replay_buffer()
   - Return new TetrisAgent instance with restored state

5. predict(observation, deterministic=True):
   - Thin wrapper around model.predict()

6. Property: timesteps_trained -> returns _total_timesteps_trained

Critical: Use DQN.load() class method, NOT DQN() then model.load(). The class method properly restores optimizer state.

Update __init__.py to export TetrisAgent.
  </action>
  <verify>
python -c "
from src.training import TetrisAgent, TrainingConfig
from src.environment import make_env, EnvConfig
env = make_env(EnvConfig(render_mode=None))()
config = TrainingConfig(max_timesteps=1000)
agent = TetrisAgent(env, config)
print('Agent created, timesteps:', agent.timesteps_trained)
env.close()
"
  </verify>
  <done>
TetrisAgent can be created with environment and config, wraps DQN properly.
  </done>
</task>

</tasks>

<verification>
All verification commands should pass:

```bash
# Module imports work
python -c "from src.training import TrainingConfig, TetrisAgent"

# Config serialization roundtrip
python -c "
from src.training import TrainingConfig
c = TrainingConfig(target_lines=100)
d = c.to_dict()
c2 = TrainingConfig.from_dict(d)
assert c2.target_lines == 100
print('Config serialization OK')
"

# Agent creation with real env
python -c "
from src.training import TetrisAgent, TrainingConfig
from src.environment import make_env, EnvConfig
env = make_env(EnvConfig())()
agent = TetrisAgent(env, TrainingConfig())
print('Agent creation OK')
env.close()
"
```
</verification>

<success_criteria>
- TrainingConfig dataclass with all DQN hyperparameters and Tetris-tuned defaults
- TetrisAgent wrapper created with DQN inside
- Agent has save_checkpoint and load_checkpoint methods
- Module exports work correctly
</success_criteria>

<output>
After completion, create `.planning/phases/02-training-core/02-01-SUMMARY.md`
</output>
