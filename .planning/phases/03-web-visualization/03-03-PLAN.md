---
phase: 03-web-visualization
plan: 03
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - src/web/training_manager.py
  - src/training/callbacks.py
autonomous: true

must_haves:
  truths:
    - "Training runs in separate process without blocking server"
    - "Metrics flow from training process to server via Queue"
    - "Board state is extracted and sent for visualization"
    - "Stop command from server terminates training gracefully"
  artifacts:
    - path: "src/training/callbacks.py"
      provides: "WebMetricsCallback for Queue-based metrics reporting"
      exports: ["WebMetricsCallback"]
    - path: "src/web/training_manager.py"
      provides: "Complete training worker implementation"
      exports: ["TrainingManager"]
  key_links:
    - from: "src/web/training_manager.py"
      to: "src/training/callbacks.py"
      via: "WebMetricsCallback in training worker"
      pattern: "WebMetricsCallback.*metrics_queue"
    - from: "src/web/training_manager.py"
      to: "src/training/agent.py"
      via: "TetrisAgent creation in worker"
      pattern: "TetrisAgent"
---

<objective>
Implement training-to-web integration with Queue-based metrics communication and board state extraction.

Purpose: Connect the existing training infrastructure (Phase 2) to the web server (Plan 01). Training runs in a subprocess, sending metrics and board state through a Queue that the server polls and broadcasts via WebSocket.

Output: Working training process that reports metrics and board state to the web server in real-time.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-web-visualization/03-RESEARCH.md

# Prior plan summaries
@.planning/phases/03-web-visualization/03-01-SUMMARY.md

# Existing training infrastructure
@src/training/train.py
@src/training/agent.py
@src/training/config.py
@src/training/callbacks.py
@src/environment/tetris_env.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create WebMetricsCallback for Queue-based metrics reporting</name>
  <files>src/training/callbacks.py</files>
  <action>
Add `WebMetricsCallback` class to `src/training/callbacks.py`:

```python
class WebMetricsCallback(BaseCallback):
    """Callback that sends metrics and board state to web server via Queue.

    Designed for use with multiprocessing.Queue to communicate training
    progress to the web visualization server.

    Message types sent:
    - metrics: Periodic training statistics
    - board: Current board state for visualization
    - episode: Episode completion data
    """

    def __init__(
        self,
        metrics_queue,
        command_queue,
        update_freq: int = 100,
        board_update_freq: int = 10,
        verbose: int = 0,
    ):
        """Initialize web metrics callback.

        Args:
            metrics_queue: multiprocessing.Queue for sending metrics to server.
            command_queue: multiprocessing.Queue for receiving commands from server.
            update_freq: Send metrics every N steps.
            board_update_freq: Send board state every N steps (more frequent for smooth viz).
            verbose: Verbosity level.
        """
        super().__init__(verbose)
        self.metrics_queue = metrics_queue
        self.command_queue = command_queue
        self.update_freq = update_freq
        self.board_update_freq = board_update_freq

        # Track episode stats
        self.episode_rewards = []
        self.episode_lines = []
        self.current_episode_reward = 0
        self.current_episode_lines = 0
        self.episode_count = 0
        self.best_lines = 0

    def _on_step(self) -> bool:
        """Called after each environment step.

        Returns:
            True to continue training, False to stop.
        """
        # Check for stop command (non-blocking)
        try:
            while not self.command_queue.empty():
                cmd = self.command_queue.get_nowait()
                if cmd.get("command") == "stop":
                    if self.verbose > 0:
                        print("Received stop command from web server")
                    return False
        except:
            pass

        # Track reward
        rewards = self.locals.get("rewards", [0])
        self.current_episode_reward += rewards[0] if rewards else 0

        # Track lines from info
        infos = self.locals.get("infos", [{}])
        for info in infos:
            lines = info.get("lines", info.get("lines_cleared", 0))
            if lines > self.current_episode_lines:
                self.current_episode_lines = lines

        # Send board state frequently for smooth visualization
        if self.n_calls % self.board_update_freq == 0:
            self._send_board_state()

        # Send metrics periodically
        if self.n_calls % self.update_freq == 0:
            self._send_metrics()

        # Check for episode end
        dones = self.locals.get("dones", [False])
        if dones[0]:
            self._on_episode_end()

        return True

    def _send_board_state(self):
        """Extract and send current board state."""
        try:
            # Access the unwrapped environment to get board state
            env = self.training_env.envs[0]
            # Navigate through wrappers to get base Tetris env
            unwrapped = env
            while hasattr(unwrapped, 'env'):
                unwrapped = unwrapped.env

            # Extract playable board area (20 rows x 10 cols)
            # tetris-gymnasium board includes padding: [0:20, 4:-4]
            if hasattr(unwrapped, 'board'):
                board = unwrapped.board[0:20, 4:-4].tolist()
                self.metrics_queue.put({
                    "type": "board",
                    "board": board,
                })
        except Exception as e:
            if self.verbose > 0:
                print(f"Error sending board state: {e}")

    def _send_metrics(self):
        """Send current training metrics."""
        avg_reward = 0
        if self.episode_rewards:
            recent = self.episode_rewards[-100:]  # Last 100 episodes
            avg_reward = sum(recent) / len(recent)

        self.metrics_queue.put({
            "type": "metrics",
            "timesteps": self.num_timesteps,
            "episode_count": self.episode_count,
            "current_score": self.current_episode_reward,
            "lines_cleared": self.current_episode_lines,
            "avg_reward": avg_reward,
            "best_lines": self.best_lines,
        })

    def _on_episode_end(self):
        """Handle episode completion."""
        self.episode_count += 1
        self.episode_rewards.append(self.current_episode_reward)
        self.episode_lines.append(self.current_episode_lines)

        if self.current_episode_lines > self.best_lines:
            self.best_lines = self.current_episode_lines

        # Send episode completion data
        self.metrics_queue.put({
            "type": "episode",
            "episode": self.episode_count,
            "reward": self.current_episode_reward,
            "lines": self.current_episode_lines,
        })

        # Reset for next episode
        self.current_episode_reward = 0
        self.current_episode_lines = 0

    def _on_training_start(self):
        """Reset tracking at training start."""
        self.episode_rewards = []
        self.episode_lines = []
        self.current_episode_reward = 0
        self.current_episode_lines = 0
        self.episode_count = 0
        self.best_lines = 0

        # Send initial status
        self.metrics_queue.put({
            "type": "status",
            "status": "running",
        })

    def _on_training_end(self):
        """Notify when training ends."""
        self.metrics_queue.put({
            "type": "status",
            "status": "stopped",
        })
```

Also update the `__init__.py` exports if needed.
  </action>
  <verify>
    ```bash
    python -c "from src.training.callbacks import WebMetricsCallback; print('WebMetricsCallback OK')"
    ```
  </verify>
  <done>
    WebMetricsCallback class added with Queue-based metrics reporting and board state extraction.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement complete training worker in TrainingManager</name>
  <files>src/web/training_manager.py</files>
  <action>
Update `src/web/training_manager.py` to implement the complete `_training_worker`:

```python
"""Process-based training management for web visualization."""

from multiprocessing import Process, Queue
from typing import Optional, Dict, Any
import traceback


class TrainingManager:
    """Manages training in a separate process with Queue-based communication.

    Training runs in an isolated process to avoid blocking the async web server.
    Communication happens via two Queues:
    - metrics_queue: Training -> Server (metrics, board state, status)
    - command_queue: Server -> Training (stop command)
    """

    def __init__(self):
        """Initialize training manager with communication queues."""
        self.metrics_queue: Queue = Queue()
        self.command_queue: Queue = Queue()
        self.process: Optional[Process] = None
        self.status: str = "stopped"

    def start_training(self, config_dict: Dict[str, Any]) -> bool:
        """Start training in a subprocess.

        Args:
            config_dict: Training configuration as dictionary.
                Keys: target_lines, max_timesteps, checkpoint_dir, etc.

        Returns:
            True if training started, False if already running.
        """
        if self.is_running():
            return False

        # Clear queues from any previous run
        self._clear_queues()

        self.process = Process(
            target=self._training_worker,
            args=(config_dict, self.metrics_queue, self.command_queue),
            daemon=True,  # Terminate with parent
        )
        self.process.start()
        self.status = "running"
        return True

    def stop_training(self) -> None:
        """Stop training gracefully, with timeout and forced termination."""
        if not self.is_running():
            return

        self.status = "stopping"

        # Send stop command
        self.command_queue.put({"command": "stop"})

        # Wait for graceful shutdown
        self.process.join(timeout=5.0)

        # Force terminate if still running
        if self.process.is_alive():
            self.process.terminate()
            self.process.join(timeout=2.0)

        self.status = "stopped"

    def is_running(self) -> bool:
        """Check if training process is alive."""
        return self.process is not None and self.process.is_alive()

    def _clear_queues(self) -> None:
        """Clear any stale messages from queues."""
        for q in [self.metrics_queue, self.command_queue]:
            try:
                while not q.empty():
                    q.get_nowait()
            except:
                pass

    @staticmethod
    def _training_worker(
        config_dict: Dict[str, Any],
        metrics_queue: Queue,
        command_queue: Queue,
    ) -> None:
        """Training worker function - runs in separate process.

        Args:
            config_dict: Training configuration dictionary.
            metrics_queue: Queue for sending metrics to server.
            command_queue: Queue for receiving commands from server.
        """
        # Import inside worker to avoid pickle issues
        try:
            from src.environment import make_env, EnvConfig
            from src.training.agent import TetrisAgent
            from src.training.config import TrainingConfig
            from src.training.callbacks import WebMetricsCallback, LinesTrackingCallback
            from stable_baselines3.common.callbacks import CallbackList

            # Create config from dict
            training_config = TrainingConfig(
                target_lines=config_dict.get("target_lines"),
                max_timesteps=config_dict.get("max_timesteps", 100000),
                checkpoint_dir=config_dict.get("checkpoint_dir", "./checkpoints"),
                checkpoint_freq=config_dict.get("checkpoint_freq", 10000),
            )

            # Create environment (headless for training, but we extract board state)
            env_config = EnvConfig(render_mode=None)
            env = make_env(env_config)()

            # Create or load agent
            from pathlib import Path
            checkpoint_path = Path(training_config.checkpoint_dir)
            checkpoint_path.mkdir(parents=True, exist_ok=True)
            latest_checkpoint = checkpoint_path / "latest"

            if latest_checkpoint.exists():
                agent = TetrisAgent.load_checkpoint(latest_checkpoint, env)
                metrics_queue.put({
                    "type": "info",
                    "message": f"Resumed from checkpoint ({agent.timesteps_trained} steps)",
                })
            else:
                agent = TetrisAgent(env, training_config)

            # Set up callbacks
            web_callback = WebMetricsCallback(
                metrics_queue=metrics_queue,
                command_queue=command_queue,
                update_freq=100,
                board_update_freq=10,
                verbose=1,
            )

            lines_tracker = LinesTrackingCallback(verbose=0)

            callback_list = CallbackList([web_callback, lines_tracker])

            # Calculate remaining timesteps
            remaining = training_config.max_timesteps - agent.timesteps_trained
            if remaining <= 0:
                metrics_queue.put({
                    "type": "status",
                    "status": "stopped",
                    "message": "Training already complete",
                })
                return

            # Send training started status
            metrics_queue.put({
                "type": "status",
                "status": "running",
            })

            # Train
            agent.train(remaining, callback=callback_list)

            # Save checkpoint
            agent.save_checkpoint(checkpoint_path / "latest")
            agent.save_checkpoint(checkpoint_path / "final")

            metrics_queue.put({
                "type": "status",
                "status": "stopped",
                "message": f"Training complete ({agent.timesteps_trained} total steps)",
            })

        except Exception as e:
            # Send error to server
            metrics_queue.put({
                "type": "error",
                "error": str(e),
                "traceback": traceback.format_exc(),
            })
            metrics_queue.put({
                "type": "status",
                "status": "stopped",
            })
```
  </action>
  <verify>
    ```bash
    python -c "from src.web.training_manager import TrainingManager; tm = TrainingManager(); print('TrainingManager OK')"
    ```
  </verify>
  <done>
    TrainingManager has complete _training_worker that creates agent, runs training with WebMetricsCallback, and handles checkpoint save/load.
  </done>
</task>

<task type="auto">
  <name>Task 3: Test training process integration</name>
  <files>None (verification only)</files>
  <action>
Create a quick integration test to verify training process works:

```bash
# Test 1: Verify imports work
python -c "
from src.web.training_manager import TrainingManager
from src.training.callbacks import WebMetricsCallback
print('Imports OK')
"

# Test 2: Start training briefly and verify metrics flow
python -c "
import time
from src.web.training_manager import TrainingManager

tm = TrainingManager()
config = {'max_timesteps': 500, 'target_lines': None}  # Very short training

print('Starting training...')
tm.start_training(config)

# Wait a moment for training to produce metrics
time.sleep(3)

# Check for metrics
metrics_received = 0
while not tm.metrics_queue.empty():
    msg = tm.metrics_queue.get_nowait()
    print(f'Received: {msg.get(\"type\", \"unknown\")}')
    metrics_received += 1

print(f'Total messages received: {metrics_received}')

# Stop training
tm.stop_training()
print(f'Status: {tm.status}')
print('Integration test passed' if metrics_received > 0 else 'WARNING: No metrics received')
"
```

Run existing tests to ensure no regressions:
```bash
python -m pytest tests/ -x -q --ignore=tests/training/test_training.py
```

Note: Ignoring test_training.py as it has the slow baseline test.
  </action>
  <verify>
    ```bash
    python -c "from src.web.training_manager import TrainingManager; from src.training.callbacks import WebMetricsCallback; print('Integration OK')"
    ```
  </verify>
  <done>
    Training process starts, sends metrics via Queue, and can be stopped gracefully. No test regressions.
  </done>
</task>

</tasks>

<verification>
All tasks complete when:
1. WebMetricsCallback sends metrics, board state, and status via Queue
2. TrainingManager._training_worker runs complete training loop
3. Board state extraction works (20x10 array via env.unwrapped.board)
4. Stop command terminates training gracefully
5. Existing tests pass
</verification>

<success_criteria>
- Training runs in separate process (no event loop blocking)
- Metrics flow: training -> Queue -> server (ready for WebSocket broadcast)
- Board state extracted correctly for visualization
- Graceful stop with timeout and forced termination
- TRAIN-07: Headless training still works
- TRAIN-08: Board state available for visual mode
</success_criteria>

<output>
After completion, create `.planning/phases/03-web-visualization/03-03-SUMMARY.md`
</output>
